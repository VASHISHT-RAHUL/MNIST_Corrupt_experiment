

Hypothesis 1: How does attention models work in deep learning?
The goal here is to understand the concept of attention models
In these set of experiments we generate a dataset such that a bag of instances is assigned a single label. 




Hypothesis 2: Does Adagrad perform layerwise training?


Hypothesis 3: How does the noise affects generalization performance in overparameterized deep models?  

MNIST Corrupt Experiment


|true training data  | Corr Training Data | Test Accuracy | Test Accuracy 0-1 | Total Training Data |   Training Accuracy |
|:------------------:|:------------------:|:-------------:|:-----------------:|:-------------------:|:-------------------:|
| 100                | 47335              |            14 |               69  |              47435  |                 100 |
| 500                | 47335              |            19 |               90  |              47835  |                 100 |
| 1000               | 47335              |            19 |               92  |              48335  |                 100 |
| 2000               | 47335              |            20 |               95  |              49335  |                 100 | 
| 4000               | 47335              |            20 |               97  |              51335  |                 100 |
| 6000               | 47335              |            20 |               97  |              53335  |                 100 |
| 8000               | 47335              |            20  |               98  |              55335  |                 100 |
| 12665              | 47335              |           20  |               99  |              60000  |                 100 |


